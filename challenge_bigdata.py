# -*- coding: utf-8 -*-
"""challenge_bigdata.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/194Q7qLHn47yNjIc8pQFpsXKJpLuMxn4-
"""

#installation de la biblithÃ©qe pyspark
!pip3 install pyspark

from pyspark.context import SparkContext
#creation of SparkContext
sc = SparkContext.getOrCreate()

from pyspark.sql import SparkSession
#creation of SparkSession
spark = SparkSession.builder.appName("commentaire hespress").getOrCreate()

spark

from google.colab import drive
drive.mount('/content/gdrive')

#read data and create dataframe
commentDF = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("/content/gdrive/My Drive/Data/comments_marocains-du-monde.csv")

#prints out the schema in the tree format.
commentDF.printSchema()

#set data types - cast the data in columns to match schema
commentDF = commentDF .withColumn("postId", commentDF["postId"].cast("string")) \
  .withColumn("comment", commentDF["comment"].cast("string")) \
  .withColumn("score", commentDF["score"].cast("int")) \
  .withColumn("topic", commentDF["topic"].cast("string"))

#prints out the new schema
commentDF.printSchema()

#showing top 5 rows
commentDF.show(5)

"""**Data analysis**"""

from pyspark.sql.functions import col

#counting the number of each score given
commentDF.groupBy("score").count().orderBy(col("count").desc()).show()

commentDF.groupBy("comment").avg("score").orderBy(col("avg(score)")).show(10)

"""**Data cleaning & preprocessing for Machine learning**

"""

#select the important columns to attend our goal
to_select = ['score', 'comment']
df = commentDF.select(to_select)
df.show(5)

#Returns a new DataFrame omitting rows with null values.
df = df.dropna(how='any')

df = df.filter(df['score'] != 0)

#Computes basic statistics for our columns.
df.describe().show()

#data type
df.dtypes

def convert(x):
    if x > 0:
        return 'Good'
    else:
        return 'Bad'

from pyspark.sql.types import *

from pyspark.sql.functions import pandas_udf, PandasUDFType

from pyspark.sql.functions import udf
#defining fuction that needs to register in the database library and use it on SQL as regular functions.
udfConvert = udf(convert, StringType())

#creating of a new column "Sentiment"
df2 = df.withColumn("Sentiment", udfConvert('score'))

#prints the new schema
df2.printSchema()

#show the new dataframe
df2.show()

from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer
# tokenizer
tokenizer = RegexTokenizer(inputCol="comment", outputCol="words")
# stop words remover
swRemover = StopWordsRemover(inputCol="words", outputCol="filtered")
# bag of words count
countVectors = CountVectorizer(inputCol="filtered", outputCol="features",
                              vocabSize=10000, minDF=5)

from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer
pipeline = Pipeline(stages=[tokenizer, swRemover, countVectors])
# Fit the pipeline to data
pipelineFit = pipeline.fit(df2)
data = pipelineFit.transform(df2)
data.show(5)

#counting our data
data.count()

#counting the data with good sentiment
data.filter(data['Sentiment']=='Good').count()

15129/18643

"""so the majority of the data are positive . More than 81% of scores are good ones

"""

data2 = data.select(['Sentiment', 'features'])
data2.show(5)

def convertNum(x):
    if x == 'Good':
        return 0
    else:
        return 1

udfConvertNum = udf(convertNum, IntegerType())
#add new column label generated by the function convertNum
data3 = data2.withColumn("label", udfConvertNum('Sentiment'))

data3.count()

from pyspark.sql.functions import *

#specify the number of train data
train = data3.limit(11000)

#save the rest as test data
test = data3.subtract(train)

"""**Machine Learning : Logistic Regression**"""

from pyspark.ml.classification import LogisticRegression

# Create initial LogisticRegression model, only use features as input
lr = LogisticRegression(labelCol="label", featuresCol="features", maxIter=10)

# Train model with Training Data
lrModel = lr.fit(train)

# Make predictions on test data using the transform() method.
# LogisticRegression.transform() will only use the 'features' column.
predictions = lrModel.transform(test)

selected = predictions.select("label", "prediction", "probability")
display(selected)

from pyspark.ml.evaluation import BinaryClassificationEvaluator

# Evaluate model
evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction")
evaluator.evaluate(predictions)

"""Diagnostics & Model improvement : Logistic Regression Grid Search Cross Validation :"""

from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

# Create ParamGrid for Cross Validation
paramGrid = (ParamGridBuilder()
             .addGrid(lr.regParam, [0.01, 0.5, 2.0])
             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])
             .addGrid(lr.maxIter, [1, 5, 10])
             .build())

# Create 5-fold CrossValidator
cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)

# Run cross validations
cvModel = cv.fit(train)

# Use test set to measure the accuracy of our model on new data
predictions2 = cvModel.transform(test)

# cvModel uses the best model found from the Cross Validation
# Evaluate best model
evaluator.evaluate(predictions2)